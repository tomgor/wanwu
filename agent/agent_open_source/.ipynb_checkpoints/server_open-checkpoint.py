import requests
import json
import os
import anyio
from openai import OpenAI

from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.graph import StateGraph,END,START
from typing import List, Dict, Optional,Annotated
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages
from langchain_core.messages import ToolMessage, HumanMessage
from langgraph.checkpoint.memory import MemorySaver
import io
from datetime import datetime
from flask import  stream_with_context, Response, request,jsonify
from flask import Flask
from flask_cors import CORS
from logging_config import setup_logging
from bing_plus import *
import configparser
from langchain.requests import RequestsWrapper
#from langchain.tools.openapi.utils.openapi_utils import OpenAPISpec,openapi_spec_to_tools
#from langchain.utilities.openapi import OpenAPISpec
#from langchain.chains.openai_functions.openapi import openapi_spec_to_tools

#from langchain.tools.openapi import OpenAPISpec, openapi_spec_to_tools

logger_name = 'agent'
app_name = os.getenv("LOG_FILE")
logger = setup_logging(app_name, logger_name)
logger.info(logger_name + '---------LOG_FILEï¼š' + repr(app_name))


app = Flask(__name__)
CORS(app, supports_credentials=True)



config = configparser.ConfigParser()
config.read('config.ini',encoding='utf-8')

BING_DAYS_LIMIT =  float(config["BING"]["BING_DAYS_LIMIT"])
BING_RESULT_LEN =  int(config["BING"]["BING_RESULT_LEN"])
BING_TOP_K =  int(config["BING"]["BING_TOP_K"])
BING_THRESHOLD =  float(config["BING"]["BING_THRESHOLD"])
BING_SENTENCE_SIZE =  int(config["BING"]["BING_SENTENCE_SIZE"])
BING_TIME_OUT =  float(config["BING"]["BING_TIME_OUT"])
TARGET_SUCCESS = int(config["BING"]["TARGET_SUCCESS"])
LLM_MODEL_NAME = config["MODELS"]["default_llm"]
CODE_SCHEMA = config["CODE"]["CODE_SCHEMA"]




def get_access_token():
    
    APP_ID = '2eee328bec434b26bd730247e652cd32'  
    API_KEY = '9ddf3d1cf32d4611ae9c2c4fbeba8a92'
    SECRET_KEY = '6b2dc478ac37473783e10e99e32810d6'
    """
    ä½¿ç”¨ API Keyï¼ŒSecret Key è·å–access_tokenï¼Œæ›¿æ¢ä¸‹åˆ—ç¤ºä¾‹ä¸­çš„åº”ç”¨API Keyã€åº”ç”¨Secret Key
    """
    url = f"https://maas-api.ai-yuanjing.com/openapi/service/v1/oauth/{APP_ID}/token"

    payload = json.dumps("")
    headers = {
        "Content-Type": "application/json",
    }
    payload = json.dumps(
        {
            "grant_type": "client_credentials",
            "client_id": API_KEY,
            "client_secret": SECRET_KEY,
        }
    )

    response = requests.request(
        "POST", url, headers=headers, data=payload, verify=False
    )
    return response.json().get("data")["access_token"]





os.environ["ARK_API_KEY"] = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IjAwYWM5NjJkLTMxNDItNGYxNy05YjAxLWJkMDQ2MjRhZmI1MCIsInRlbmFudElEcyI6bnVsbCwidXNlclR5cGUiOjAsInVzZXJuYW1lIjoid2FuZ3l5NjAzIiwibmlja25hbWUiOiLnjovoibPpmLMiLCJidWZmZXJUaW1lIjoxNzQ4MzQ5MDE5LCJleHAiOjIzNzkwNjE4MTksImp0aSI6IjMxNjU0ZjdiMmFhZjQ2NDI5Mzc0MzFkN2Q4NThhNWJiIiwiaWF0IjoxNzQ4MzQxNTE5LCJpc3MiOiIwMGFjOTYyZC0zMTQyLTRmMTctOWIwMS1iZDA0NjI0YWZiNTAiLCJuYmYiOjE3NDgzNDE1MTksInN1YiI6ImtvbmcifQ.w1cMbQWTQZIbxrz7DYt14xFSAt8BQCpyjFFhUO6JrwY"



os.environ["TAVILY_API_KEY"] = '************'


# å®šä¹‰è‡ªå·±çš„å¤§æ¨¡å‹
class MyChatModel:
    def __init__(self, api_key, model="yuanjing-70b-functioncall", temperature=0):
        self.api_key = api_key
        self.model = model
        self.temperature = temperature
        self.headers = {
            "Content-Type":"application/json",
            "Authorization":f"Bearer {api_key}"
        }
        self.api_url = "https://maas-api.ai-yuanjing.com/openapi/compatible-mode/v1/chat/completions"
        
        
    def bind_tools(self, tools):
        """ç»‘å®šå·¥å…·åˆ°æ¨¡å‹"""
        self.tools = tools
        return self

    def chat(self, messages):
        # ç³»ç»Ÿæç¤ºè¯
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·æ¥å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
    è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
    1. å¦‚æœç”¨æˆ·é—®é¢˜å¯ä»¥é€šè¿‡å·¥å…·ï¼ˆå¦‚è·å–å¤©æ°”ã€æœç´¢å®æ—¶ä¿¡æ¯ï¼‰æ›´å¥½åœ°å›ç­”ï¼Œè¯·è°ƒç”¨ç›¸åº”å·¥å…·
    2. å¦‚æœç”¨æˆ·é—®é¢˜ä¸éœ€è¦å·¥å…·å°±èƒ½å›ç­”ï¼Œè¯·ç›´æ¥ç»™å‡ºç­”æ¡ˆ
    3. å·¥å…·è°ƒç”¨å¿…é¡»éµå¾ªæä¾›çš„å·¥å…·è§„èŒƒ
    4. é¿å…ä¸å¿…è¦çš„å·¥å…·è°ƒç”¨ï¼Œåªæœ‰åœ¨ç¡®å®éœ€è¦æ—¶æ‰ä½¿ç”¨å·¥å…·
    å¯ç”¨å·¥å…·ï¼š
    - get_weather: è·å–æŒ‡å®šåŸå¸‚çš„å½“å‰å¤©æ°”
    - search: æœç´¢å®æ—¶ä¿¡æ¯ï¼Œé€‚ç”¨äºå›ç­”éœ€è¦æœ€æ–°æ•°æ®çš„é—®é¢˜"""

        messages_list = []
        for message in messages:
            if isinstance(message, HumanMessage):
                role = "user"
            else:
                role = "assistant"
            if message.content != "":
                messages_list.append({"role": role, "content": message.content})
        messages = [{"role": "system", "content": system_prompt}] + messages_list

        # å‘é€èŠå¤©è¯·æ±‚
        data = {
            "model": self.model,
            "messages": messages,
            "temperature": self.temperature,
            "tools": self.tools if hasattr(self, 'tools') else []
        }
        try:
            response = requests.post(
                self.api_url,
                headers = self.headers,
                data=json.dumps(data)
            )
            response.raise_for_status()
            response = response.json()
            print('å¤§æ¨¡å‹å›ç­”ï¼š',response)
            return response["choices"][0]["message"]
        except requests.exceptions.RequestException as e:
            print(f"Error:{e}")
            return None


# å®šä¹‰çŠ¶æ€ç±»å‹
class AgentState(TypedDict):
    # å†å²æ¶ˆæ¯åˆ—è¡¨
    messages: Annotated[list, add_messages]



# é…ç½®ä¼šè¯id
config_fun = {"configurable": {"thread_id": "1"}}


@app.route("/agent",methods=['POST'])
def agent_start():
    @stream_with_context
    def generate():
        try:

            data = request.get_json()
            logger.info('å…¥å‚æ˜¯request_params: '+ json.dumps(data, ensure_ascii=False))
            #åŸºæœ¬å‚æ•°
            question = data.get("input")
            stream = data.get("stream")
            history = data.get("history")
            auth_header = request.headers.get('Authorization')
            userId = request.headers.get('X-Uid')
            function_call = data.get("function_call",False)


            #å¤§æ¨¡å‹å‚æ•°
            model = data.get("model")
            model_url = data.get("model-url")
            system_role = data.get("system_role")
            
            
            do_sample = data.get("do_sample")
            temperature = data.get("temperature")
            repetition_penalty = data.get("repetition_penalty")
            frequency_penalty = data.get("frequency_penalty")
            top_p = data.get("top_p")
            top_k = data.get("top_k")
            max_tokens = data.get("max_tokens")
            do_think = data.get("do_think")

            #æœç´¢å‚æ•°
            auto_citation = data.get("auto_citation",True)
            use_search = data.get("use_search")
            need_search_list = data.get("need_search_list")
            #bing_top_k = data.get("bing_top_k")
            #bing_target_success = data.get("bing_target_success",10)
            #bing_time_out = data.get("bing_time_out",3.0)


            #ä»£ç è§£é‡Šå™¨å‚æ•°
            use_code = data.get("use_code")
            file_name = data.get("file_name")
            upload_file_url = data.get("upload_file_url")


            #ragå‚æ•°
            #chitchat = data.get("chitchat",True)
            kn_params = data.get("kn_params",{})
            use_know = data.get("use_know")

            #å…¶ä»–æ’ä»¶å‚æ•°
            plugin_list = data.get("plugin_list")
            #extend_params = data.get("extend_params")

            knowledgebase_name = ''
            if kn_params:
                knowledgebase_name = kn_params['knowledgeBase']
                threshold = kn_params['threshold']
                topk = kn_params['topk']


            used_rag = False
            #å¦‚æœä¼ å‚æœ‰çŸ¥è¯†åº“ åˆ™å…ˆèµ°rag
            if use_know:
                print('-----------å…ˆèµ°çŸ¥è¯†åº“å›ç­”')
                url = "https://maas-api.ai-yuanjing.com/openapi/knowledge/stream/search" 

                payload = {
                    "knowledgeBase": knowledgebase_name,
                    "question": question,
                    "threshold": threshold,
                    "topK": topk,
                    "stream": True,
                    "chitchat": False,
                    "history": [],
                    "auto_citation":auto_citation
                }
                if auth_header and auth_header.startswith('Bearer '):
                    token = auth_header.split(' ')[1]
                #print('tokenæ˜¯:',token)
                access_token = token

                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {access_token}"
                }

                # å‘é€POSTè¯·æ±‚
                response = requests.post(url, headers=headers, data=json.dumps(payload),stream=True)
                if response.status_code == 200:
                    first_line_checked = False
                    for line in response.iter_lines():
                        if line:
                            try:
                                decoded_line = line.decode("utf-8").strip()
                                if decoded_line.startswith("data:"):
                                    decoded_line = decoded_line[len("data:"):].strip()
                                data = json.loads(decoded_line)
                                if not first_line_checked:
                                    first_line_checked = True
                                    if data["data"]["searchList"]:
                                        print('çŸ¥è¯†åº“å¯å›ç­”')
                                        used_rag = True
                                    print('çŸ¥è¯†åº“å›ç­”ä¸äº†')
                                if used_rag:
                                    answer = {
                "code": 0,
                "message": "success",
                "response": "",
                "gen_file_url_list": [

                ],
                "history": [],
                "finish": 0,
                "usage": {
                    "prompt_tokens": 3272,
                    "completion_tokens": 79,
                    "total_tokens": 3351
                },
                "search_list": []
                }
                                    answer['response']=data["data"]["output"]
                                    answer['code']=data["code"]
                                    answer['finish']=data["finish"]
                                    answer['message']=data["message"]
                                    answer['search_list']=data["data"]["searchList"]
                                    answer['history']=data["history"]
                                    
                                    yield f"data: {json.dumps(answer, ensure_ascii=False)}\n\n"
                            except Exception as e:
                                yield f"data: {json.dumps({'error': str(e)}, ensure_ascii=False)}\n\n"
            if not used_rag: 

                if function_call:

                    print('---------èµ°function_callæ¨¡å¼')


                    #1.å®šä¹‰æœç´¢æ’ä»¶
                    def net_search(query: str,model:str,model_url:str) -> str:
                        """Answer with some knowledge that can be searched online"""
                        try:
                            loop = None
                            loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(loop)
                            days_limit = BING_DAYS_LIMIT
                            bing_top_k = BING_TOP_K
                            bing_time_out = BING_TIME_OUT
                            bing_target_success = 10
                            bing_result_len = BING_RESULT_LEN
                            auto_citation = False
                            model = LLM_MODEL_NAME
                            task = start_async_search(loop, query,bing_top_k,bing_time_out,bing_target_success,bing_result_len, model,days_limit, auto_citation)                     
                            result = loop.run_until_complete(task)
                            bing_prompt, bing_search_list, = result
                            context = "".join(item["snippet"] for item in bing_search_list)

                            print('ä¸Šä¸‹æ–‡æ˜¯:',context)
                            #now = datetime.now()
                            #date_str = now.strftime("%Y-%m-%d")
                            ACCESS_TOKEN = get_access_token()
                            TOOL_PROMPT_TEMPLATE = '''ä½ æ˜¯ä¸€ä¸ªå®æ—¶è”ç½‘çš„æ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼Œç›®å‰ä½äºä¸­å›½ï¼Œä½ çš„ä¸»è¦ä»»åŠ¡æ˜¯å‚è€ƒä¸‹åˆ—ä»äº’è”ç½‘æœç´¢åˆ°çš„ç½‘é¡µä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

                    ## å‚è€ƒä¿¡æ¯
                    ```
                    {context}
                    ```

                    ## ç”¨æˆ·é—®é¢˜
                    ```
                    {question}
                    ```



                    ## è¾“å‡ºè¦æ±‚
                    ```
                    - ç­”æ¡ˆä¸­ä¸è¦å‡ºç°"æ ¹æ®æ‚¨æä¾›çš„ä¿¡æ¯"ã€"æ ¹æ®æä¾›çš„ä¿¡æ¯"ã€"æ ¹æ®å‚è€ƒä¿¡æ¯"ç­‰ä¹‹ç±»çš„è¯æœ¯ã€‚
                    - å›ç­”å¿…é¡»ç´§æ‰£ç”¨æˆ·é—®é¢˜ï¼Œå›ç­”è¦å®Œæ•´è¯¦ç»†ï¼Œå¿…è¦æ—¶ç»™å‡ºåˆ†æè¿‡ç¨‹ã€‚
                    - ç»“åˆç”¨æˆ·é—®é¢˜æƒ…å†µï¼Œä¸èƒ½é—æ¼å¯¹äºå½“å‰é—®é¢˜çš„å…³é”®ä¿¡æ¯ï¼Œä¾‹å¦‚é’ˆå¯¹å¤©æ°”ç±»é—®é¢˜ï¼Œéœ€è¦ç»™å‡ºå¤©æ°”çŠ¶æ€ã€æ¸©åº¦ç­‰å…³é”®ä¿¡æ¯ã€‚
                    - æ¶‰åŠè®¡ç®—çš„ï¼Œè¯·ç»™å‡ºåˆ†æè¿‡ç¨‹ã€‚
                    - å¦‚æœå‚è€ƒä¿¡æ¯æ²¡æœ‰æ‰¾åˆ°ç­”æ¡ˆï¼Œç›´æ¥åŸºäºè‡ªèº«çŸ¥è¯†å›ç­”ï¼Œä¼˜å…ˆå›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä½†æ˜¯ä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
                    - ä¸è¦è¾“å‡ºä¸ç”¨æˆ·é—®é¢˜æ— å…³çš„å†…å®¹ã€‚
                    - è¦æ³¨æ„åŒºåˆ†å¼€å§‹æ—¥æœŸã€ç»“æŸæ—¥æœŸå’Œå½“å‰æ—¥æœŸï¼Œå¦‚æœç»“æŸæ—¥æœŸæ—©äºå½“å‰æ—¥æœŸï¼Œè¯´æ˜å·²ç»ç»“æŸäº†ã€‚
                    -  ç­”æ¡ˆæ­£æ–‡ä¸­ä¸è¦è§£é‡Šä¸ºä»€ä¹ˆä½¿ç”¨å“ªäº›å‚è€ƒä¿¡æ¯ã€‚

                    ```'''
                            prompt = TOOL_PROMPT_TEMPLATE.format(question=query,context=context[:3000])
                            llm = ChatOpenAI(
                                model=model,
                                api_key=ACCESS_TOKEN,
                                base_url=model_url,
                                temperature=0.7,
                                streaming=False
                            )

                            # æ„é€ æ¶ˆæ¯
                            messages = [
                                HumanMessage(content=prompt)
                            ]
                            response = llm(messages)
                            return response
                        finally:
                            loop.close()

                    #2.å®šä¹‰codeæ’ä»¶
                    def code(query: str,upload_file_url:str) -> str:
                        """Used for code generation or code execution to solve problems."""
                        url = "http://192.168.0.172:7257/api/cal"
                        if upload_file_url:
                            need_file = True
                        else:
                            need_file = False
                        payload = {
                            "input": query,
                            "need_file": need_file,  # Change this based on your requirements
                            "history": [],
                            "upload_file_url": upload_file_url,
                            "language":"ä¸­æ–‡",
                            "stream":False
                        }

                        headers = {
                            "Content-Type": "application/json"
                        }

                        # å‘é€POSTè¯·æ±‚
                        #response = requests.post(url, headers=headers, data=json.dumps(payload))
                        response = requests.post(url, data=json.dumps(payload), headers=headers)
                        if response.status_code == 200:
                            data = json.loads(response.text)
                            content = data["data"]["choices"][0]["message"]["content"]
                            return content
                        else:
                            print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                            print(response.text)

                    #3.å¤©æ°”æ’ä»¶                
                    def get_weather(city: str) -> str:
                        if city == "åŒ—äº¬":
                            return f"{city}å¤©æ°”æ™´æœ—ï¼Œæ¸©åº¦28åº¦ã€‚"
                        return f"{city}å¤©æ°”å¤šäº‘ï¼Œå¾®é£ã€‚"


                    #spec = OpenAPISpec.from_file("amp.json")
                    #requests_wrapper = RequestsWrapper()
                    #tools_plugin = openapi_spec_to_tools(spec, requests_wrapper=requests_wrapper)


                    # å®šä¹‰å·¥å…·çš„json-schemaæè¿°
                    tools = [
                        {
                            "type": "function",
                            "function": {
                                "name": "get_weather",
                                "parameters": {
                                    "type": "object",
                                    "properties": {
                                        "city": {"type": "string", "description": "åŸå¸‚åç§°"}
                                    },
                                    "required": ["city"]
                                },
                                "description": "è·å–æŒ‡å®šåŸå¸‚çš„å½“å‰å¤©æ°”"
                            }
                        },
                        {
                            "type": "function",
                            "function": {
                                "name": "net_search",
                                "parameters": {
                                    "type": "object",
                                    "properties": {
                                        "query": {"type": "string", "description": "éœ€è¦æœç´¢çš„å†…å®¹"}
                                    },
                                    "required": ["query","model","model_url"]
                                },
                                "description": "æœç´¢å®æ—¶ä¿¡æ¯ï¼Œé€‚ç”¨äºå›ç­”éœ€è¦æœ€æ–°æ•°æ®çš„é—®é¢˜"
                            }
                    }
                    ]
                    llm = MyChatModel(api_key=os.environ["ARK_API_KEY"], model=model,temperature=0).bind_tools(tools)

                    #tools.extend(tools_plugin)
                    print('toolsæœ‰å“ªäº›:',tools)
                    # æ·»åŠ AgentèŠ‚ç‚¹
                    def run_agent(state: AgentState):
                        message = state["messages"][-1]
                        if not isinstance(message, HumanMessage) and message.content != "":
                            return
                        return {"messages": [llm.chat(state["messages"])]}



                    def run_tool(state: AgentState):
                        message = state["messages"][-1]
                        outputs = []
                        if hasattr(message, "tool_calls") and len(message.tool_calls) > 0:
                            for tool_call in message.tool_calls:
                                name = tool_call["name"]
                                args = tool_call["args"]
                                # æ ¹æ®å·¥å…·çš„åå­—è°ƒç”¨ä¸åŒçš„å·¥å…·
                                if name == "get_weather":
                                    function = get_weather
                                if name == "net_search":
                                    function = net_search
                                # æ‰§è¡Œå·¥å…·å‡½æ•°
                                observation = function(**args)
                                outputs.append(
                                    ToolMessage(
                                        content=observation,
                                        name=name,
                                        tool_call_id=tool_call["id"],
                                    )
                                )
                        return {"messages": outputs}





                    # åˆ›å»ºå›¾ç»“æ„
                    workflow = StateGraph(AgentState)

                    # æ·»åŠ èŠ‚ç‚¹
                    workflow.add_node("agent", run_agent)
                    workflow.add_node("tool", run_tool)
                    workflow.set_entry_point("agent")  # æ­£ç¡®è®¾ç½®å…¥å£èŠ‚ç‚¹


                    # è·¯ç”±å‡½æ•°ï¼Œæ ¹æ®æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨åˆ¤æ–­
                    def route(state: AgentState):
                        messages = state["messages"][-1]
                        if hasattr(messages, "tool_calls") and len(messages.tool_calls) > 0:
                            return "tool"
                        return "end"

                    # æ·»åŠ æ¡ä»¶è¾¹
                    workflow.add_conditional_edges(
                        "agent",
                        route,
                        {
                            "tool": "tool",
                            "end": END
                        }
                    )
                    # æ·»åŠ æ™®é€šè¾¹
                    workflow.add_edge("tool", "agent")

                    # åˆå§‹åŒ–è®°å¿†æ¨¡å—
                    memory = MemorySaver()

                    # ç¼–è¯‘å›¾
                    #graph = workflow.compile(checkpointer=memory)
                    graph = workflow.compile()

                    # åˆå§‹åŒ–å¤§æ¨¡å‹å¹¶ç»‘å®šå·¥å…·

                    question = 'é—®é¢˜æ˜¯:'+question+'\n'+'ä»¥ä¸‹æ˜¯æ’ä»¶å¯èƒ½ç”¨åˆ°çš„å‚æ•°ï¼š'+'\n'+'model:'+model+'\n'+'model_url:'+model_url+'\n'+'upload_file_url:'+upload_file_url
                    answer = {
"code": 0,
"message": "success",
"response": "",
"gen_file_url_list": [],
"history": [],
"finish": 0,
"usage": {
    "prompt_tokens": 3272,
    "completion_tokens": 79,
    "total_tokens": 3351
},
"search_list": []

}
                    initial_state = {
    "input": question,
    "messages": [HumanMessage(content=question)],
    # å¦‚ä½¿ç”¨ checkpointï¼Œè¿˜å¯ä»¥ä¼ å…¥ "thread_id" ç­‰
}                
                    
                    
                    for event in graph.stream(initial_state):                        
                        print("ğŸ”¹ Event:", event)
                        
                        if "agent" in event and "messages" in event["agent"]:
                            message = event["agent"]["messages"][-1]
                            if hasattr(message, "tool_calls"):
                                for call in message.tool_calls:
                                    tool_name = call["name"]
                                    print(f"ğŸ› ï¸ Agentå†³å®šè°ƒç”¨å·¥å…·ï¼š{tool_name}")
                                    yield f"[Agent è°ƒç”¨å·¥å…·]: {tool_name}"
                        if "tool" in event and "messages" in event["tool"] and event["tool"]["messages"]: 
                            content = event["tool"]["messages"][-1].content
                            match = re.search(r"content='(.*?)'", content)
                            content = match.group(1)
                            print('toolçš„è¾“å‡º:',content)
                            yield content
                            
                            #print(event["messages"][-1]["content"])
                            #yield event["messages"][-1]["content"]
                    #result = graph.invoke({"messages": [{"role": "user", "content": question}]}, config_fun)
                    #for output in graph.stream({"messages": [HumanMessage(content=question)]},config=config_fun):
                        #yield f"data:{json.dumps(output, ensure_ascii=False)}\n\n"
                        
                        
                    
                    #response = result["messages"][-1].content
                    #yield response
                else:
                    #èµ°éfunction callæµç¨‹ï¼Œactioné€»è¾‘ï¼Œæ‰€æœ‰åŠŸèƒ½å‡ä¸ºå·¥å…·ä¼ å…¥actionä¸€å¹¶è¾“å‡º
                    print('------------èµ°action')
                    action_url = "http://localhost:4802/agent/action"
                    headers = {
                        "Content-Type": "application/json"
                    }
                    code_schema =         {
            "api_schema": {
                "info": {
                    "description": "ç”¨äºç”Ÿæˆä»£ç ã€è·‘ä»£ç ã€é€šè¿‡ç¼–å†™ä»£ç å¤„ç†æ–‡ä»¶",
                    "title": "ä»£ç è§£é‡Šå™¨API",
                    "version": "1.0.0"
                },
                "openapi": "3.0.0",
                "paths": {
                    "/api/cal": {
                        "post": {
                            "description": "ç”¨äºå›ç­”ç”¨æˆ·çš„ä»£ç ç±»é—®é¢˜ï¼Œä¾‹å¦‚ç”Ÿæˆä»£ç ã€è·‘ä»£ç ã€é€šè¿‡ç¼–å†™ä»£ç å¤„ç†æ–‡ä»¶ ",
                            "summary":"ç”Ÿæˆä»£ç è·‘ä»£ç å¤„ç†æ–‡ä»¶",
                            "operationId": "CodeGeneration",
                            "requestBody": {
                                "content": {
                                    "application/json": {
                                        "schema": {
                                            "properties": {
                                                "input": {
                                                    "description": "ç”¨æˆ·æå‡ºçš„é—®é¢˜",
                                                    "type": "string"
                                                },
                                                "upload_file_url": {
                                                    "description": "æ–‡ä»¶ä¸‹è½½é“¾æ¥",
                                                    "type": "string"
                                                },
                                                "is_use_api_output":{
                                                    "description":"æ˜¯å¦ç›´æ¥ä½¿ç”¨apiç»“æœè¿›è¡Œè¾“å‡ºï¼Œé»˜è®¤ä¸ºTrue",
                                                    "type":"string"
                                                },
                                                "stream":{
                                                    "description":"æ˜¯å¦æµå¼å›ç­”,é»˜è®¤ä¸ºTrue",
                                                    "type":"string"
                                                }   
                                            },
                                            "required": [
                                                "input",
                                                "is_use_api_output",
                                                "stream"
                                            ],
                                            "type": "object"
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "servers": [
                    {
                        "url": "http://192.168.0.172:7257"
                    }
                ]
            }
        }

                    netsearch_schema =         {
            "api_schema": {
                "info": {
                    "description": "ç”¨äºé€šè¿‡ç½‘ç»œæŸ¥è¯¢æœç´¢å®æ—¶é—®é¢˜çš„ç›¸å…³ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜",
                    "title": "ç½‘ç»œæœç´¢",
                    "version": "1.0.0"
                },
                "openapi": "3.0.0",
                "paths": {
                    "/net_search": {
                        "post": {
                            "description": "ç”¨äºé€šè¿‡ç½‘ç»œæŸ¥è¯¢æœç´¢å®æ—¶é—®é¢˜çš„ç›¸å…³ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜",
                            "summary":"ç½‘ç»œæœç´¢ä¿¡æ¯",
                            "operationId": "netsearch",
                            "requestBody": {
                                "content": {
                                    "application/json": {
                                        "schema": {
                                            "properties": {
                                                "query": {
                                                    "description": "ç”¨æˆ·æå‡ºçš„é—®é¢˜",
                                                    "type": "string"
                                                },
                                                "model": {
                                                    "description": "é€‰æ‹©çš„æ¨¡å‹åç§°",
                                                    "type": "string"
                                                },
                                                "is_use_api_output":{
                                                    "description":"æ˜¯å¦ç›´æ¥ä½¿ç”¨apiç»“æœè¿›è¡Œè¾“å‡ºï¼Œé»˜è®¤ä¸ºTrue",
                                                    "type":"string"
                                                },
                                                "model_url":{
                                                    "description":"æ¨¡å‹è°ƒç”¨url",
                                                    "type":"string"
                                                },
                                                "stream":{
                                                    "description":"æ˜¯å¦æµå¼å›ç­”,é»˜è®¤ä¸ºTrue",
                                                    "type":"string"
                                                }                                                
                                            },
                                            "required": [
                                                "query",
                                                "is_use_api_output",
                                                "model",
                                                "model_url",
                                                "stream"
                                            ],
                                            "type": "object"
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "servers": [
                    {
                        "url": "http://192.168.0.126:1990"
                    }
                ]
            }
        }
                    
                    
                    if use_code:
                        plugin_list.append(code_schema)
                    if use_search:
                        plugin_list.append(netsearch_schema)
                    if plugin_list:
                        
                        question = 'é—®é¢˜æ˜¯:'+question+'\n'+'ä»¥ä¸‹æ˜¯æ’ä»¶å¯èƒ½ç”¨åˆ°çš„å‚æ•°ï¼š'+'\n'+'model:'+model+'\n'+'model_url:'+model_url+'\n'+'upload_file_url:'+upload_file_url

                        print('plugin_listæ˜¯ä»€ä¹ˆ:',plugin_list)
                        payload = {
                            "input":question,
                            "plugin_list":plugin_list,
                            "action_type": "qwen_agent",
                            "model_name":model,
                            "model_url":model_url
                        }
                        
                        response = requests.post(action_url, headers=headers, data=json.dumps(payload),stream=True,verify=False)
                        if response:
                            
                            answer = {
        "code": 0,
        "message": "success",
        "response": "",
        "gen_file_url_list": [

        ],
        "history": [],
        "finish": 0,
        "usage": {
            "prompt_tokens": 3272,
            "completion_tokens": 79,
            "total_tokens": 3351
        },
        "search_list": []
    }

                            for line in response.iter_lines(decode_unicode=True):
                                print('actionè¾“å‡ºæ˜¯ä»€ä¹ˆ:',line)
                                if line.startswith("data:"):
                                    line = line[5:]
                                    datajson = json.loads(line)
                                    answer['code'] = datajson['code']
                                    answer['message'] = datajson['msg']
                                    content_str = datajson['data']['choices'][0]['message']['content']
                                    if isinstance(content_str, dict):
                                        if "search_list" in content_str:
                                            answer['search_list'] = content_str.get('search_list')
                                        if "gen_file_url_list" in content_str:
                                            answer['gen_file_url_list'] = content_str.get('gen_file_url_list')

                                        answer['response'] = content_str.get('response')
                                    else:
                                        answer['response'] = datajson['data']['choices'][0]['message']['content']
                                    if datajson["data"]["choices"][0]["finish_reason"] == '':
                                        answer['finish']=0
                                    else:
                                        answer['finish']=1



                                    answer['usage']['completion_tokens'] = datajson["data"]["usage"]['completion_tokens']
                                    answer['usage']['prompt_tokens'] = datajson["data"]["usage"]['prompt_tokens']
                                    answer['usage']['total_tokens'] = datajson["data"]["usage"]['total_tokens']


                                    yield f"data:{json.dumps(answer,ensure_ascii=False)}\n"

                        else:
                            
                            print('------æ²¡å‘½ä¸­ä»»ä½•å·¥å…·èµ°çº¯å¤§æ¨¡å‹å›ç­”')
                            llm = ChatOpenAI(
                                model_name=model,
                                streaming=True,
                                base_url=model_url,
                                openai_api_key=os.environ["ARK_API_KEY"],
                            )
                            answer = {
        "code": 0,
        "message": "success",
        "response": "",
        "gen_file_url_list": [],
        "history": [],
        "finish": 0,
        "usage": {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        },
        "search_list": []
    }
                            for chunk in llm.stream(question):
                                # chunk æ˜¯ä¸€ä¸ª AIMessageChunk æˆ– ChatMessageChunkï¼Œè¦è½¬æˆå­—ç¬¦ä¸²
                                if hasattr(chunk, "content"):
                                    print('å¤§æ¨¡å‹è¾“å‡ºæ˜¯:',chunk)
                                    answer['response'] = chunk.content
                                if hasattr(chunk, "response_metadata"):
                                    if 'finish_reason' in chunk.response_metadata and chunk.response_metadata['finish_reason']=='stop':
                                        answer['finish']=1
                                if hasattr(chunk, "usage_metadata") and chunk.usage_metadata is not None:
                                    answer['usage']['prompt_tokens'] = chunk.usage_metadata['input_tokens'] if 'input_tokens' in chunk.usage_metadata else 0
                                    answer['usage']['completion_tokens'] = chunk.usage_metadata['output_tokens'] if 'output_tokens' in chunk.usage_metadata else 0
                                    answer['usage']['total_tokens'] = chunk.usage_metadata['total_tokens'] if 'total_tokens' in chunk.usage_metadata else 0
                                yield f"data:{json.dumps(answer,ensure_ascii=False)}\n"
                            #answer["finish"] = 1
                            #yield f"data:{json.dumps(answer, ensure_ascii=False)}\n"

                            


                    else:
                        print('------æœªé…ç½®ä»»ä½•å·¥å…·èµ°çº¯å¤§æ¨¡å‹å›ç­”')
                        llm = ChatOpenAI(
                            model_name=model,
                            streaming=True,
                            base_url=model_url,
                            openai_api_key=os.environ["ARK_API_KEY"],
                        )
                        answer = {
    "code": 0,
    "message": "success",
    "response": "",
    "gen_file_url_list": [],
    "history": [],
    "finish": 0,
    "usage": {
        "prompt_tokens": 3272,
        "completion_tokens": 79,
        "total_tokens": 3351
    },
    "search_list": []
}
                        for chunk in llm.stream(question):
                            # chunk æ˜¯ä¸€ä¸ª AIMessageChunk æˆ– ChatMessageChunkï¼Œè¦è½¬æˆå­—ç¬¦ä¸²
                            if hasattr(chunk, "content"):
                                print('å¤§æ¨¡å‹è¾“å‡ºæ˜¯:',chunk)
                                answer['response'] = chunk.content
                                
                                
                                if hasattr(chunk, "response_metadata"):
                                    if 'finish_reason' in chunk.response_metadata and chunk.response_metadata['finish_reason']=='stop':
                                        answer['finish']=1
                                if hasattr(chunk, "usage_metadata") and chunk.usage_metadata is not None:
                                    answer['usage']['prompt_tokens'] = chunk.usage_metadata['input_tokens'] if 'input_tokens' in chunk.usage_metadata else 0
                                    answer['usage']['completion_tokens'] = chunk.usage_metadata['output_tokens'] if 'output_tokens' in chunk.usage_metadata else 0
                                    answer['usage']['total_tokens'] = chunk.usage_metadata['total_tokens'] if 'total_tokens' in chunk.usage_metadata else 0
                                yield f"data:{json.dumps(answer,ensure_ascii=False)}\n"



                    
        except Exception as e:
            logger.exception("âŒ å¤„ç†è¯·æ±‚å¤±è´¥ï¼š")
            error_data = {
                "code": 1,
                "message": str(e),
                "response": "",
                "finish": 1
            }
            yield f"data:{json.dumps(error_data, ensure_ascii=False)}\n"

    return Response(generate(), mimetype="text/event-stream")
    
    
    
if __name__ == '__main__':
    logger.info("agent_server start")
    app.run(host='0.0.0.0', port=7258, threaded=False,debug=False)