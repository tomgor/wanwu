import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import json
import requests
import chardet
from flask import Flask, jsonify, request, make_response
# from knowledge_base_utils import *
from flask_cors import CORS
from bs4 import BeautifulSoup
from urllib.parse import unquote_plus 
import argparse
import re

from logging_config import setup_logging


TEMP_URL_FILES_DIR = os.path.join(os.path.dirname(__name__), 'temp_url_files')
os.makedirs(TEMP_URL_FILES_DIR, exist_ok=True)


logger_name='url_single'
app_name = os.getenv("LOG_FILE")
logger = setup_logging(app_name,logger_name)
logger.info(logger_name+'---------LOG_FILE：'+repr(app_name))

app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})

app.config['JSON_AS_ASCII'] = False
app.config['JSONIFY_MIMETYPE'] ='application/json;charset=utf-8'

MINIO_URL = 'http://localhost:15000/upload'
MINIO_BUCKET_NAME = 'rag-doc'


def clean_text(text):
    """清除文本中的特殊字符和多余的空白，以及HTML标签。"""
    patterns = [
        r'\xa0+', r'\u3000', r'\t+', r'\r+', r'\n+',   # 清除特殊空白字符和多行换行符
        r'<[/]?b>|&gt;|&lt;'                        # 清除HTML标签
    ]
    for pattern in patterns:
        text = re.sub(pattern, '', text)
    return text.strip()

def is_text_garbled(text):
    non_displayable_char_ratio = len(re.findall(r'[^\x20-\x7E\u4e00-\u9fff]', text)) / len(text)
    return non_displayable_char_ratio > 0.5    


#解析服务
@app.route('/url_pra', methods=["POST","GET"])
def url_add():
    a = ''
    data = request.json
    #url = unescape_unicode(data.get('url', '')) 
    url = data.get('url')
    logger.info(f"入参是: {data}")
    url = unquote_plus(url) 
    
    logger.info(f"The value of url is: {url}")
    if not url:
        return jsonify({'error': 'URL is required'}), 400
    try:


        headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36'
    }

        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        encoding = chardet.detect(response.content)['encoding']
        response.encoding = encoding if encoding else 'utf-8'# 设置编码，确保中文显示正常
        soup = BeautifulSoup(response.content, 'html.parser')
        a = clean_text(soup.get_text())
        logger.info(f"解析出的内容是: {a}")


        #解析有问题，在这里返回
        if is_text_garbled(a):
            response_data = {  
                "file_name": '',
                "old_name":url,# 添加原始name和文件名到响应数据中  
                "response_info": {
                    "code": 1,
                    "message": "该网站不支持抓取解析"                
                }   
            }
            logger.info(f"codewrong: {url}")
            json_str = json.dumps(response_data, ensure_ascii=False)
            response = make_response(json_str) 
            return response
        if len(a) < 30:
            response_data = {  
                "file_name": '',
                "old_name":url,# 添加原始name和文件名到响应数据中  
                "response_info": {
                    "code": 1,
                    "message": "该网站不支持抓取解析"                
                }   
            }
            logger.info(f"short: {url}")
            json_str = json.dumps(response_data, ensure_ascii=False)
            response = make_response(json_str) 
            return response

        b = ''
        logger.info(f"normal: {url}")
        title_tag = soup.find('title')
        logger.info(f"原来标题是: {title_tag.text}")
        c = title_tag.text
        b = c.replace('|', '_').replace(':', '_').replace(' ', '_')
        
        if len(b) >= 50:
            b = b[:30]
        else:
            b = b
        title_text = b if title_tag else '无标题'
        logger.info(f"标题是: {title_text}")

        name = title_text+'.txt'
        logger.info(f"解析文件名是: {name}")
        file_path = os.path.join(TEMP_URL_FILES_DIR, name)
        with open(file_path, 'w', encoding='utf-8') as file:
            file.write(a)
        with open(file_path, 'r', encoding='utf-8') as file:  
            content = file.read() 
        file_size = os.path.getsize(file_path)
        file_size_kb = file_size / 1024
        response_data = {  
            "file_name": title_text,
            "old_name":url,# 添加原始name和文件名到响应数据中  
            "file_size":file_size_kb,
            "response_info": {
                "code": 0,
                "message": "解析成功"                
            }  
        }
        
        json_str = json.dumps(response_data, ensure_ascii=False)
        response = make_response(json_str)       
    except Exception as e:
        logger.info(f"error: {str(e)}")
        logger.info(f"error类型: {type(str(e))}")
        if "HTTPSConnectionPoolstr" in str(e):
            response_data = {  
                "file_name": '',
                "old_name":url,# 添加原始name和文件名到响应数据中  
                "response_info": {
                    "code": 1,
                    "message": "该网站不支持抓取解析"                
                }  
            } 
        else:            
            response_data = {  
                "file_name": '',
                "old_name":url,# 添加原始name和文件名到响应数据中  
                "response_info": {
                    "code": 1,
                    "message": "该网站不支持抓取解析"                
                }  
            }
        
        json_str = json.dumps(response_data, ensure_ascii=False)
        response = make_response(json_str)
        logger.info(f"错误在: {response_data}")
    return response,200



#解析出内容入库
@app.route('/url_insert', methods=["POST","GET"])
def url_insert_data():
    logger.info('进入入库')
    data = request.json
    file_name = data.get('file_name')
    # overlap_size = data.get('overlap_size',0.0)
    # sentence_size = data.get('sentence_size', 300)    
    # chunk_type = data.get('chunk_type', 'split_by_default') 
    # user_id = data.get("userId")
    # kb_name = data.get("knowledgeBase")
    # is_enhanced = data.get("is_enhanced", 'false')
    # separators = data.get("separators", ['。'])
    task_id = data.get("task_id")
    logger.info('*********!!!')
    try:
        name = file_name+'.txt'
        old_file_path = os.path.join(TEMP_URL_FILES_DIR, name)
        new_file_path = os.path.join(TEMP_URL_FILES_DIR, task_id+'.txt')
        os.rename(old_file_path, new_file_path)       
        link = ''
        try:
            with open(new_file_path, "rb") as file:
                files_minio = {"file": file}
                data = {
                    "file_name":task_id,
                    "bucket_name":MINIO_BUCKET_NAME,
                }
                response = requests.post(MINIO_URL, files=files_minio,data=data)
                if response.status_code == 200:
                    
                    link = response.json()["download_link"]
                    logger.info(f"minio sucess: {link}")
                else:
                    logger.info(f"minio wrong")


            response_data = {  
                "file_name": task_id + '.txt',
                "download_link":link,
                "response_info":{
                "code":0,
                "message":"入库成功"}
            }
            logger.info(f"response_data: {response_data}")
            json_str = json.dumps(response_data, ensure_ascii=False)
            response = make_response(json_str)
        except Exception as e:
            import traceback
            logger.error("====> split_chunks error %s" % e)
            logger.error(traceback.format_exc())
            logger.error(repr(e))
            
            
    except Exception as e:
        logger.info(f"error: {str(e)}")
        return jsonify({'error': str(e)}), 500
    logger.info(f"insert sucess: {response}")
    return response,200




if __name__ == '__main__':
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--port", type=int)
    args = parser.parse_args()
    app.run(host='0.0.0.0', port=args.port)
    

